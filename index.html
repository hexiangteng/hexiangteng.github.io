<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.40.1" />
  <meta name="author" content="Xiangteng He">
  <!--<meta name="description" content="Research Scientist">-->
  <link rel="alternate" hreflang="en-us" href="/">
  <meta name="theme-color" content="#0095eb">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono"> 
  <link rel="stylesheet" href="styles.css">
  <script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-54472398-1', 'auto');
    ga('require', 'eventTracker');
    ga('require', 'outboundLinkTracker');
    ga('require', 'urlChangeTracker');
    ga('send', 'pageview');
  </script>
  <script async src="//www.google-analytics.com/analytics.js"></script>
  <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Xiangteng He">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Xiangteng He">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">
  <link rel="canonical" href="/">
  <meta property="og:site_name" content="Xiangteng He">
  <meta property="og:url" content="/">
  <meta property="og:title" content="Xiangteng He">
  <meta property="og:locale" content="en-us">
  <meta property="og:updated_time" content="2021-02-08T00:00:00&#43;00:00">

  <title>Xiangteng He</title>
  
  
  <script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?cbc33b0bd659ddd771c6572a53e8991a";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();

</script>


</head>


<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71" >
  <nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
    <div class="container">  
      <div class="navbar-header"> 
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
                data-target=".navbar-collapse" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        
        <a class="navbar-brand" href="/">Xiangteng He</a>
      </div>
      
      <div class="collapse navbar-collapse"> 
        <ul class="nav navbar-nav navbar-right">  

          <li class="nav-item">
            <a href="/#Biography" data-target="#Biography">      
              <span>Biography</span>     
            </a>
          </li>

          <li class="nav-item">
            <a href="/#News" data-target="#News">      
              <span>News</span>     
            </a>
          </li>

          <li class="nav-item">
            <a href="/#Publications" data-target="#Publications">
              <span>Publications</span>   
            </a>
          </li>

          <li class="nav-item">
            <a href="/#Honors" data-target="#Honors">
              <span>Honors</span>   
            </a>
          </li>
          
          <!--<li class="nav-item">
            <a href="/#Competitions" data-target="#Competitions">
              <span>Competitions</span>   
            </a>
          </li>-->

         <!-- <li class="nav-item">
            <a href="/#Media Coverage" data-target="#Media Coverage">
              <span>Media Coverage</span>   
            </a>
          </li>-->
       
        </ul>
      </div>

    </div>
  </nav>


    <span id="homepage" style="display: none"></span><div class="container">
     
<div class="row">
  <div class="col-xs-12 col-md-4">
    <div id="profile">

      
      <div class="portrait" itemprop="image" style="background-image: url('hexiangteng.jpg');">
      </div>
      
    </div>
  </div>

  <div class="col-xs-12 col-md-8" itemprop="description">
   </br>
          <h3>
          Xiangteng He 何相腾
          </h3>
          <p>
          University of British Columbia (UBC)
          </p>
          <p>
          Email: xiangteng.he AT ubc.ca
          </p>
          <h3 style="padding-top:-5px"></h3>
            <!--<a href="CV-Xiangteng He.pdf" >CV</a> / -->
            <a href="https://scholar.google.com/citations?user=37RO0eYAAAAJ&hl=En" >Google Scholar</a>

    </div>
  </div>


  
  
<section id="Biography" class="home-section">
    <div class="container">
      <h1>Biography</h1>
      <div class="col-xs-12">
        <p style="text-align:justify; text-justify:inter-ideograph;">
        I am a Postdoctoral Researcher in the Department of Computer Science at UBC, working with Prof. Leonid Sigal.
        Previously, I served as an Assistant Research Professor at Peking University. Prior to academia, I spent a wonderful year in Alibaba DAMO Academy as a Senior Algorithm Engineer.
        I received Ph.D. degree from Peking University, and B.E degree from Nankai University.
        My research interests include fine-grained multi-modal analysis, vision-language models, and their applications (e.g., medical imaging, biodiversity monitoring).

        
        </p>
         <p style="text-align:justify; text-justify:inter-ideograph; color: red">
        Looking for strong graduate/undergraduate students to collaborate. Please reach out if you are interested.
        </p>
        
      </div>
    </div>
  </section>


 
  
  <!-- <section id="Call for papers" class="home-section">
    <div class="container">
      <h1 style="color:red">Call for papers</h1>
      <div class="col-xs-12">
          <br>
          <a href="https://sites.google.com/view/fgvc12/home">The 12th Workshop on Fine-Grained Visual Categorization</a>, <a href="https://cvpr.thecvf.com/">CVPR 2025</a>.
         This year, FGVC12 will have two paper tracks and a nectar track:
         </br>
        <li>
         1. Proceeding track: 8-page papers that will appear in the official CVPR workshop proceedings
        </li>
        <li>
        2. Non-archival track: 4-page extended abstracts
      </li>
        <li>
        3. Nectar track: already published work
      </li>
        <br>
        Papers in track 1 and 2 will be reviewed. They will showcase new work, along with applications of fine-grained learning. Submission will be via CMT. Please take a look at the <a href="https://sites.google.com/view/fgvc12/submission">different deadlines</a>.
      </br>
      
      <img src="papers/FGVC12.jpeg" alt="" />
  
      </div>
    </div>
  </section> -->

  <section id="News" class="home-section">
    <div class="container">
      <h1>News</h1>
      <div class="col-xs-12">

        <li>
          2025-12-22: Co-organizing the <a href="https://sites.google.com/view/fgvc13/home"> FGVC13 workshop</a> at CVPR 2026.
        </li> 

        <li>
          2025-10-22: Poster paper at ICCV 2025 <a href="https://ali-vilab.github.io/ICE-Bench-Page/"> ICE-Bench: A Unified and Comprehensive Benchmark for Image Creating and Editing</a>.
        </li>

        <li>
          2025-07-28: Very honored to serve as SPC in AAAI 2026.
        </li>

        <li>
          2025-06-25: One paper is accepted by ICCV 2025. Congrats to Yulin Pan!
        </li>

        <li>
          2025-05-03: Very honored to serve as SPC in ECAI 2025.
        </li>

        <li>
            2025-01-03: Very honored to serve as SPC in IJCAI 2025.
          </li>

        <li>
          2024-12-21: Co-organizing the <a href="https://sites.google.com/view/fgvc12/home"> FGVC12 workshop</a> at CVPR 2025.
        </li> 

         <li>
          2024-12-10: One paper is accepted by AAAI 2025. Congrats to <a href="https://roc-ng.github.io/">Peng Wu</a> !
        </li>

         <li>
          2024-09-02: One paper is accepted by IEEE TIP. Congrats to Hongbo Sun!
        </li>

         <li>
          2024-08-25: One paper is accepted by ACM TOMM. Congrats to Zhaoda Ye!
        </li>

         <li>
          2024-04-17: One paper is accepted by IJCAI 2024. Congrats to Hongbo Sun!
        </li>

        <li>
          2024-04-12: One paper is accepted by ACM TOMM. Congrats to Ruoyan Pi!
        </li>

         <li>
          2024-02-21: One paper is accepted by IEEE TIP. Congrats to <a href="https://roc-ng.github.io/">Peng Wu</a> !
        </li>
        
        <li>
          2024-01-17:  Very honored to win <a href="https://link.springer.com/journal/11633/updates/26630820">MIR Outstanding Reviewer Award in 2023</a>.
        </li> 
        
        <li>
          2023-12-25: Very honored to receive the Academic Innovation Award from Tencent WeChat (腾讯微信犀牛鸟专项研究计划学术创新奖)</a>.
        </li> 
        
        <li>
          2023-12-24: Co-organizing the FGVC11 workshop at CVPR 2024.
        </li> 

         <li>
          2023-12-09: One papers is accepted by AAAI 2024. Congrats to Yanzhe Chen!
        </li> 

        <li>
          2023-12-01: One papers is accepted by ICDE 2024. Congrats to Hulingxiao He!
        </li> 
        
        <li>
          2023-10-16: One papers is accepted by IEEE TMM 2023. Congrats to Hongbo Sun!
        </li> 
        
         <li>
          2023-07-26: Four papers are accepted by ACM MM 2023. Congrats to Hongbo Sun, Zijun Deng, and Yanzhe Chen!
        </li> 
        
        <li>
          2023-07-14: One paper is accepted by ICCV 2023. Congrats to Yulin Pan!
        </li> 
        
        <li>
          2023-03-07: One paper is accepted by ACM TOMM. Congrats to Zijun Deng!
        </li> 
        
        <li>
          2023-02-28: One paper is accepted by CVPR 2023. Congrats to HsiaoYuan Hsu!
        </li> 
        
        <li>
          2023-01-10: Co-organizing the <a href="https://sites.google.com/view/fgvc10/home">FGVC10</a> workshop at CVPR 2023.
        </li>
        
         <li>
          2023-01-03: One paper is accepted by ACM TOMM. Congrats to Duoduo Feng!
        </li> 

       
             <li>
            2022-11-29: Invited to serve as Area Chair in ICME 2023.
          </li>       
      <!--     
          <li>
            2022-10-25: Awarded Young Elite Scientists Sponsorship Program by CAST（中国科协青年人才托举工程项目）.
          </li> 
          
          <li>
            2022-07-20: I give a talk “Fine-grained Cross-media Categorization and Retrieval” at <a href="http://chinamm.csig.org.cn/2022/tutorial.html"> ChinaMM 2022 </a>.
          </li>
          
          <li>
            2022-07-08: One paper is accepted by <a href="https://cje.ejournal.org.cn/">Chinese Journal of Electronics (《电子学报》英文版)</a>. Congrats to Zhaoda Ye!
          </li>
          
           <li>
            2022-07-01: One paper is accepted by ACM MM 2022. Congrats to Hongbo Sun!
          </li>
          
          <li>
            2022-03-31: One paper is accepted by ACM SIGIR 2022.
          </li>
          
          <li>
            2022-01-05: Co-organizing the <a href="https://sites.google.com/view/fgvc9/home">FGVC9</a> workshop at CVPR 2022.
          </li>
          
          <li>
            2021-07-04: Two papers are accepted by ACM MM 2021. Congrats to Zhen Han and <a href="https://roc-ng.github.io/">Peng Wu</a> !
          </li>
 
          <li>
            2021-05-17: I give a talk "<a href="https://cs.pku.edu.cn/info/1020/3025.htm">Fine-grained Visual Categorization and Retrieval</a>" at Peking Unversity.
          </li>
  
          <li>
            2021-01-31: I attend the <a href="https://www.ccf.org.cn/Focus/2021-01-19/721992.shtml">award ceremony of CCF (China Computer Federation) Outstanding Doctoral Dissertation Award</a>.
          </li>
  
          <li>
            2020-12-10: Very honored to serve as SPC in IJCAI 2021.
          </li>
  
          <li>
            2020-12-03: I give a talk "Use What You Have: Multi-modal Information in Fine-grained Analysis" at <a href="https://sites.google.com/view/webfg2020/home"> WebFG Workshop in ACCV 2020 </a>.
          </li>
  -->
        
       

      </div>
    </div>
  </section>

  
  <section id="Publications" class="home-section">
    <div class="container">
      <h1>Publications</h1>
            <div class="col-xs-12">
              <h2>2025</h2>
              <ol>
                <li>
                    <p align="justify">
                      <b>Xiangteng He</b>#, Shunsuke Sakai#, Kun Yuan, Nicolas Padoy, Tatsuhito Hasegawa, Leonid Sigal, 
                      "DSeq-JEPA: Discriminative Sequential Joint-Embedding Predictive Architecture", 
                      Preprint, 
                      2025.
                      <a href="https://arxiv.org/abs/2511.17354">[arXiv]</a>
                      <a href="https://github.com/SkyShunsuke/DSeq-JEPA">[Project]</a>
                      <a href="https://mp.weixin.qq.com/s/NVXZfd_4GFxszLBz2lcwrw">[Introduction]</a>
                  </p>
                </li>

                <li>
                  <p align="justify">
                    Kun Yuan, Min Woo Sun, Zhen Chen, Alejandro Lozano, <b>Xiangteng He</b>, Shi Li, Nassir Navab, Xiaoxiao Sun, Nicolas Padoy, Serena Yeung-Levy, 
                    "From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature", 
                    Preprint, 
                    2025.
                    <a href="https://arxiv.org/abs/2512.02566">[arXiv]</a>
                </p>
              </li>

                <li>
                  <p align="justify">
                    Jiayun Luo#, Wan-Cyuan Fan#, Lyuyang Wang, <b>Xiangteng He</b>, Tanzila Rahman, Purang Abolmaesumi, Leonid Sigal, 
                    "To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models", 
                    Preprint, 
                    2025.
                    <a href="https://arxiv.org/abs/2510.08510">[arXiv]</a>
                    <a href="https://davidhalladay.github.io/diysink_demo/">[Project]</a>
                </p>
              </li>

                <li>
                  <p align="justify">
                    Yulin Pan, <b>Xiangteng He*</b>, Chaojie Mao, Zhen Han, Zeyinzi Jiang, Jingfeng Zhang, Yu Liu, 
                    "ICE-Bench: A Unified and Comprehensive Benchmark for Image Creating and Editing", 
                    International Conference on Computer Vision (<b>ICCV</b>), 
                    pp. 16586-16596, Honolulu, Hawai'i, USA, October 19 – 23, 2025.
                    <a href="https://ali-vilab.github.io/ICE-Bench-Page/">[Project]</a>
                    <a href="https://huggingface.co/datasets/ali-vilab/ICE-Bench">[Dataset]</a>
                    <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Pan_ICE-Bench_A_Unified_and_Comprehensive_Benchmark_for_Image_Creating_and_ICCV_2025_paper.pdf">[PDF]</a>
                    <a href="https://openaccess.thecvf.com/content/ICCV2025/supplemental/Pan_ICE-Bench_A_Unified_ICCV_2025_supplemental.pdf">[supp]</a>

                </p>
              </li>

                <li>
                  <p align="justify">
                     Zhaoda Ye, <b>Xiangteng He</b>, Yuxin Peng, "RaT2IGen: Relation-aware Text-to-image Generation via Learnable Prompt", ACM Transactions on Multimedia Computing, Communications and Applications (<b>TOMM</b>), 
                     Vol. 21, Issue 5, Article No. 151, pp. 1 - 19, May 2025.
                     <a href="https://dl.acm.org/doi/pdf/10.1145/3726527">[PDF]</a>
                 </p>
               </li>

                 <li>
                   <p align="justify">
                      Peng Wu, Wanshun Su, <b>Xiangteng He</b>, Peng Wang, Yanning Zhang, "VarCMP: Adapting Cross-Modal Pre-Training Models for Video Anomaly Retrieval", 
                     AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 
                     Vol. 39, No. 8, pp. 8423-8431, Philadelphia, Pennsylvania, USA, February 25 – March 4, 2025.
                     <a href="https://ojs.aaai.org/index.php/AAAI/article/download/32909/35064">[PDF]</a>
                  </p>
                </li>
                
            </ol>

            
            <h2>2024</h2>
            <ol>
                <li>
                   <p align="justify">
                      Hongbo Sun, <b>Xiangteng He</b>, Jinglin Xu, Yuxin Peng, "SIM-OFE: Structure Information Mining and Object-aware Feature Enhancement for Fine-Grained Visual Categorization", IEEE Transactions on Image Processing (<b>TIP</b>), Vol. 33, pp. 5312-5326, Sep. 2024. <a href="https://ieeexplore.ieee.org/abstract/document/10684043">[PDF]</a>
                  </p>
                </li>

               <li>
                   <p align="justify">
                      Hongbo Sun, Jiahuan Zhou, <b>Xiangteng He</b>, Jinglin Xu, Yuxin Peng, "FineFMPL: Fine-grained Feature Mining Prompt Learning for Few-Shot Class Incremental Learning", International Joint Conference on Artificial Intelligence (<b>IJCAI</b>), pp. 1299-1307, Jeju, South Korea, August 3-9, 2024. 
                     <a href="https://www.ijcai.org/proceedings/2024/0144.pdf">[PDF]</a>
                     <a href="https://github.com/PKU-ICST-MIPL/FineFMPL_IJCAI2024">[Code]</a> 
                  </p>
                </li>

               <li>
                   <p align="justify">
                      Ruoyan Pi, Peng Wu, <b>Xiangteng He*</b>, Yuxin Peng, "EOGT: Video Anomaly Detection with Enhanced Object Information and Global Temporal Dependency", ACM Transactions on Multimedia Computing, Communications, and Applications (<b>TOMM</b>), Vol. 20, Issue 10, Article No. 320, pp. 1-21, Sep. 2024. 
                     <a href="https://dl.acm.org/doi/abs/10.1145/3662185">[PDF]</a>
                  </p>
                </li>

               <li>
                   <p align="justify">
                      Hulingxiao He, <b>Xiangteng He*</b>, Yuxin Peng, Zifei Shan, Xin Su, "Firzen: Firing Strict Cold-Start Items with Frozen Heterogeneous and Homogeneous Graphs for Recommendation", IEEE International Conference on Data Engineering (<b>ICDE</b>), pp. 4657-4670, Utrecht, Netherlands, May 13 - 16th, 2024. 
                     <a href="https://www.computer.org/csdl/proceedings-article/icde/2024/171500e657/1YOtR33OVcQ">[PDF]</a>
                     <a href="https://github.com/PKU-ICST-MIPL/Firzen_ICDE2024">[Code]</a>
                     <a href="https://mp.weixin.qq.com/s/q_mncGA16oDL5aJsCVZJvg">[Introduction]</a>
                  </p>
                </li>

               <li>
                   <p align="justify">
                      邓梓焌, <b>何相腾</b>, 彭宇新, 
                     "文本到视频生成：研究现状、进展和挑战", 
                     <b>电子学报</b>, 
                     Vol.46, pp. 1632-1644, May. 2024. 
                     <a href="https://jeit.ac.cn/en/article/doi/10.11999/JEIT240074?">[PDF]</a>
                  </p>
                </li>

               <li>
                   <p align="justify">
                      Peng Wu, Jing Liu, <b>Xiangteng He</b>, Yuxin Peng, Peng Wang, Yanning Zhang, "Toward Video Anomaly Retrieval From Video Anomaly Detection: New Benchmarks and Model", IEEE Transactions on Image Processing (<b>TIP</b>), Vol. 33, pp. 2213-2225, Mar. 2024. 
                     <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10471334">[PDF]</a>
                     <a href="https://github.com/Roc-Ng/VAR">[Datasets]</a>
                  </p>
                </li>

              <li>
                   <p align="justify">
                      Hongbo Sun, <b>Xiangteng He</b>, Yuxin Peng, 
                     "HCL: Hierarchical Consistency Learning for Webly Supervised Fine-Grained Recognition", 
                     IEEE Transactions on Multimedia 
                     (<b>TMM</b>), 
                     Vol.26, pp. 5108-5119, Mar. 2024. 
                     <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10308611">[PDF]</a>
                     <a href="https://github.com/PKU-ICST-MIPL/HCL_TMM2023">[Code]</a>
                     <a href="https://mp.weixin.qq.com/s/eO8hvJsAepyzJLCwoCOPSw">[Introduction]</a>
                  </p>
                </li>

              <li>
                   <p align="justify">
                      Yanzhe Chen, Huasong Zhong, <b>Xiangteng He</b>, Yuxin Peng, Jiahuan Zhou, Lele Cheng, 
                     "FashionERN: Enhance-and-Refine Network for Composed Fashion Image Retrieval", 
                     AAAI Conference on Artificial Intelligence 
                     (<b>AAAI</b>), 
                     Vol. 38, No. 2, pp. 1228-1236, Vancouver, Canada, February 20 - 27, 2024. 
                     <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27885/27795">[PDF]</a>
                     <a href="https://github.com/ChenAnno/FashionERN_AAAI2024">[Code]</a>
                  </p>
                </li>
            </ol>

              <h2>2023</h2>
              <ol>

              <li>
                   <p align="justify">
                      Hongbo Sun, <b>Xiangteng He</b>, Jiahuan Zhou, Yuxin Peng, 
                     "Fine-Grained Visual Prompt Learning of Vision-Language Models for Image Recognition", 
                     ACM Multimedia Conference 
                     (<b>ACM MM</b>), 
                     pp. 5828–5836, Ottawa, Canada, Oct.29 - Nov.3, 2023. 
                     <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612403">[PDF]</a>
                     <a href="https://mp.weixin.qq.com/s/1vjULaFy_TbpILXZBAM2nw">[Introduction]</a>
                  </p>
                </li>

              <li>
                   <p align="justify">
                      Yanzhe Chen, Huasong Zhong, <b>Xiangteng He</b>, Yuxin Peng, Lele Cheng, 
                     "Real20M: A Large-scale E-commerce Dataset for Cross-domain Retrieval", 
                     ACM Multimedia Conference 
                     (<b>ACM MM</b>), 
                     pp. 4939–4948, Ottawa, Canada, Oct.29 - Nov.3, 2023. 
                     <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612408">[PDF]</a>
                     <a href="https://github.com/ChenAnno/Real20M_ACMMM2023">[Code]</a>
                     <a href="https://github.com/ChenAnno/Real20M_ACMMM2023">[Dataset]</a>
                     <a href="https://mp.weixin.qq.com/s/6JD8-9h2BHl-61YWqnlK5w">[Introduction]</a>
                  </p>
                </li>
              
              <li>
                   <p align="justify">
                      Zijun Deng, <b>Xiangteng He</b>, Yuxin Peng, Xiongwei Zhu, Lele Cheng, 
                     "MV-Diffusion: Motion-aware Video Diffusion Model", 
                     ACM Multimedia Conference 
                     (<b>ACM MM</b>), 
                     pp. 7255-7263, Ottawa, Canada, Oct.29 - Nov.3, 2023.
                     <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612405">[PDF]</a>
                     <a href="https://mp.weixin.qq.com/s/FAlPsTsSSHEBLNWQIKNiCQ">[Introduction]</a>
                  </p>
                </li>

              <li>
                   <p align="justify">
                      Zijun Deng, <b>Xiangteng He</b>, Yuxin Peng, 
                     "Efficiency-optimized Video Diffusion Models", 
                     ACM Multimedia Conference 
                     (<b>ACM MM</b>), 
                     pp. 7295-7303, Ottawa, Canada, Oct.29 - Nov.3, 2023.
                     <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612406">[PDF]</a>
                     <a href="https://mp.weixin.qq.com/s/QKbHvqyBjlN1RxXEQ0kUmg">[Introduction]</a>
                  </p>
                </li>

                <li>
                   <p align="justify">
                      Yulin Pan, <b>Xiangteng He*</b>, Biao Gong, Yiliang Lv, Yujun Shen, Yuxin Peng, Deli Zhao*, 
                     "Scanning Only Once: An End-to-end Framework for Fast Temporal Grounding in Long Videos", 
                     International Conference of Computer Vision 
                     (<b>ICCV</b>), 
                     pp. 13767-13777, Paris, France, Oct. 2-6, 2023.
                     <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pan_Scanning_Only_Once_An_End-to-end_Framework_for_Fast_Temporal_Grounding_ICCV_2023_paper.pdf">[PDF]</a>
                     <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Pan_Scanning_Only_Once_ICCV_2023_supplemental.pdf">[SUPP]</a>
                     <a href="https://github.com/afcedf/SOONet">[Code]</a>
                  </p>
                </li>

                 <li>
                   <p align="justify">
                      HsiaoYuan Hsu, <b>Xiangteng He</b>, Yuxin Peng, 
                     "DensityLayout: Density-conditioned Layout GAN for Visual-textual Presentation Designs", 
                     International Conference on Image and Graphics 
                     (<b>ICIG</b>), 
                     pp. 187–199, Nanjing, China, Sep.22 - 24, 2023.
                     <a href="https://link.springer.com/chapter/10.1007/978-3-031-46308-2_16">[PDF]</a>
                  </p>
                </li>

                <li>
                   <p align="justify">
                      Zijun Deng, <b>Xiangteng He</b>, Yuxin Peng, 
                     "LFR-GAN: Local Feature Refinement based Generative Adversarial Network for Text-to-Image Generation", 
                     ACM Transactions on Multimedia Computing, Communications, and Applications
                     (<b>TOMM</b>), 
                     Vol. 19, Issue 6, Article No. 207, pp. 1-18, Jul. 2023.
                     <a href="https://dl.acm.org/doi/abs/10.1145/3589002">[PDF]</a>
                     <a href="https://github.com/PKU-ICST-MIPL/LFR-GAN_TOMM2023">[Code]</a>
                     <a href="https://mp.weixin.qq.com/s/zWM52FjzAVN0Jodyqotmig">[Introduction]</a>
                  </p>
                </li>

                <li>
                   <p align="justify">
                      HsiaoYuan Hsu, <b>Xiangteng He</b>, Yuxin Peng, Hao Kong, Qing Zhang, 
                     "PosterLayout: A New Benchmark and Approach for Content-Aware Visual-Textual Presentation Layout", 
                     IEEE Conference on Computer Vision and Pattern Recognition
                     (<b>CVPR</b>), 
                     pp. 6018-6026, Vancouver, Canada, Jun. 18-22, 2023.
                     <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hsu_PosterLayout_A_New_Benchmark_and_Approach_for_Content-Aware_Visual-Textual_Presentation_CVPR_2023_paper.pdf">[PDF]</a>
                     <a href="https://github.com/PKU-ICST-MIPL/PosterLayout-CVPR2023">[Code]</a>
                     <a href="http://39.108.48.32/mipl/PosterLayout/">[Dataset]</a>
                     <a href="https://mp.weixin.qq.com/s/yMLBINvS3ftJkWTuwFGDbQ">[Introduction]</a>
                  </p>
                </li>

                <li>
                   <p align="justify">
                      Duoduo Feng, <b>Xiangteng He</b>, Yuxin Peng, 
                     "MKVSE: Multimodal Knowledge Enhanced Visual-Semantic Embedding for Image-Text Retrieval", 
                     ACM Transactions on Multimedia Computing, Communications, and Applications
                     (<b>TOMM</b>), 
                     Vol. 19, Issue 5, Article No. 162, pp. 1–21, Mar. 2023.
                     <a href="https://dl.acm.org/doi/abs/10.1145/3580501">[PDF]</a>
                     <a href="https://github.com/PKU-ICST-MIPL/MKVSE-TOMM2023">[Code]</a>
                     <a href="https://mp.weixin.qq.com/s/czwMFL3WDctDqVYKangVyQ">[Introduction]</a>
                  </p>
                </li>
              </ol>


              <h2>2022</h2>
              <ol>
                 <li>
                   <p align="justify">
                      Ruoyan Pi, <b>Xiangteng He</b>, Yuxin Peng, 
                     "Weakly Supervised Video Anomaly Detection with Temporal and Abnormal Information", 
                     Chinese Conference on Pattern Recognition and Computer Vision
                     (<b>PRCV</b>), 
                     pp. 594–608, Shenzhen, China, Nov. 4-7, 2022. 
                     <a href="https://link.springer.com/chapter/10.1007/978-3-031-18913-5_46">[PDF]</a>
                  </p>
                </li>
                
                <li>
                   <p align="justify">
                      Hongbo Sun, <b>Xiangteng He</b>, Yuxin Peng, 
                     "SIM-Trans: Structure Information Modeling Transformer for Fine-grained Visual Categorization", 
                     ACM Multimedia Conference
                     (<b>ACM MM</b>), 
                     pp. 5853–5861, Lisbon, Portugal, Oct. 10-14, 2022.
                     <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548308">[PDF]</a>
                     <a href="papers/ACM MM 2022 PPT.pdf">[PPT]</a> 
                     <a href="https://github.com/PKU-ICST-MIPL/SIM-Trans_ACMMM2022">[Code]</a>
                  </p>
                </li>

                <li>
                   <p align="justify">
                      Zhaoda Ye, <b>Xiangteng He</b>, Yuxin Peng, 
                     "Unsupervised Cross-media Hashing Learning via Knowledge Graph", 
                     Chinese Journal of Electronics
                     (<b>CJE</b>), 
                     Vol. 31, No. 6, pp. 1081–1091, Oct. 2022.
                     <a href="https://ietresearch.onlinelibrary.wiley.com/doi/epdf/10.1049/cje.2021.00.455">[PDF]</a>
                  </p>
                </li>

                <li>
                   <p align="justify">
                      <b>Xiangteng He</b>#, Yulin Pan#, Mingqian Tang, Yiliang Lv, Yuxin Peng, 
                     "Learn from Unlabeled Videos for Near-duplicate Video Retrieval", 
                     ACM SIGIR Conference on Research and Development in Information Retrieval
                     (<b>ACM SIGIR</b>), 
                     pp. 1002-1011, Madrid, Spain, Jul. 11-15, 2022.
                     <a href="https://dl.acm.org/doi/abs/10.1145/3477495.3532010">[PDF]</a> 
                     <a href="papers/SIGIR 2022 PPT.pdf">[PPT]</a> 
                     <a href="https://mp.weixin.qq.com/s/u6Jd3LAUueFncZTY_pkwTA">[Introduction]</a>
                  </p>
                </li>
                </ol>


              <h2>2021</h2>
              <ol>
                <li>
                   <p align="justify">
                      Peng Wu, <b>Xiangteng He*</b>, Mingqian Tang, Yiliang Lv, Jing Liu*, 
                     "HANet: Hierarchical Alignment Networks for Video-Text Retrieval", 
                     ACM Multimedia Conference
                     (<b>ACM MM</b>), 
                     pp. 3518–3527, Chengdu, China, Oct. 20-24, 2021.
                     <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475515">[PDF]</a> 
                     <a href="https://github.com/Roc-Ng/HANet">[Code]</a>
                  </p>
                </li>

                <li>
                   <p align="justify">
                      Zhen Han, <b>Xiangteng He*</b>, Mingqian Tang, Yiliang Lv, 
                     "Video Similarity and Alignment Learning on Partial Video Copy Detection", 
                     ACM Multimedia Conference
                     (<b>ACM MM</b>), 
                     pp. 4165–4173, Chengdu, China, Oct. 20-24, 2021. 
                     <a href="https://dl.acm.org/doi/abs/10.1145/3474085.3475549">[PDF]</a> 
                     <a href="https://pvcd-vsal.github.io/vsal/">[Project]</a>
                  </p>
                </li>
              </ol>

              <h2>2020</h2>
              <ol>
                <li>
                   <p align="justify">
                      Junjie Zhao, <b>Xiangteng He</b>, Yuxin Peng, 
                     "Attribute Hierarchy based Multi-task Learning for Fine-grained Image Classification", 
                     <b>Neurocomputing</b>, 
                     Vol. 395, pp. 150-159, Jun. 2020.
                     <a href="https://www.sciencedirect.com/science/article/pii/S0925231219308938">[PDF]</a>
                  </p>
                </li>
                
                <li>
                   <p align="justify">
                      <b>Xiangteng He</b>, Yuxin Peng, 
                     "Fine-grained Visual-textual Representation Learning", 
                     IEEE Transactions on Circuits and Systems for Video Technology
                     (<b>TCSVT</b>), 
                     Vol. 30, No. 2, pp. 520-531, Feb. 2020.
                     <a href="https://ieeexplore.ieee.org/abstract/document/8611142">[PDF]</a> 
                  </p>
                </li>
              </ol>

            <h2>2019</h2>
              <ol>
                <li>
                   <p align="justify">
                      <b>Xiangteng He</b>, Yuxin Peng, Liu Xie, 
                     "A New Benchmark and Approach for Fine-grained Cross-media Retrieval", 
                     ACM Multimedia Conference
                     (<b>ACM MM</b>), 
                     pp. 1740-1748, Nice, France, Oct. 21-25, 2019.
                     <a href="https://dl.acm.org/doi/abs/10.1145/3343031.3350974">[PDF]</a> 
                     <a href="http://39.108.48.32/mipl/FGCrossNet/">[Dataset]</a> 
                     <a href="https://github.com/PKU-ICST-MIPL/FGCrossNet_ACMMM2019">[Code]</a>
                     <a href="https://mp.weixin.qq.com/s/BFp2EefZ1hC5Z3T5rcX4Zg">[Introduction]</a>
                  </p>
                </li>
                
                <li>
                   <p align="justify">
                      <b>Xiangteng He</b>, Yuxin Peng, Junjie Zhao, 
                     "Which and How Many Regions to Gaze: Focus Discriminative Regions for Fine-grained Visual Categorization", 
                     International Journal of Computer Vision
                     (<b>IJCV</b>), 
                     Vol. 127, No. 9, pp. 1235-1255, Sep. 2019.
                     <a href="https://link.springer.com/article/10.1007/s11263-019-01176-2">[PDF]</a> 
                  </p>
                </li>

                <li>
                   <p align="justify">
                      <b>Xiangteng He</b>, Yuxin Peng, Junjie Zhao, 
                     "Fast Fine-grained Image Classification via Weakly Supervised Discriminative Localization", 
                     IEEE Transactions on Circuits and Systems for Video Technology
                     (<b>TCSVT</b>), 
                     Vol. 29, No. 5, pp. 1394-1407, May. 2019.
                     <a href="https://ieeexplore.ieee.org/abstract/document/8356107">[PDF]</a> 
                     <a href="https://github.com/PKU-ICST-MIPL/WSDL_TCSVT2019">[Code]</a>
                  </p>
                </li>                
              </ol>

             <h2>2018</h2>
              <ol>
                <li>
                   <p align="justify">
                      <b>Xiangteng He</b>, Yuxin Peng, 
                     "Multi-attention Guided Activation Propagation in CNNs", 
                     Chinese Conference on Pattern Recognition and Computer Vision
                     (<b>PRCV</b>),  
                     pp. 16-27, Guangzhou, China, Nov. 23-26, 2018.
                     <a href="https://link.springer.com/chapter/10.1007/978-3-030-03335-4_2">[PDF]</a>
                  </p>
                </li>
                
                <li>
                   <p align="justify">
                      <b>Xiangteng He</b>, Yuxin Peng, 
                     "Only Learn One Sample: Fine-Grained Visual Categorization with One Sample Training", 
                     ACM Multimedia Conference
                     (<b>ACM MM</b>), 
                     pp. 1372-1380, Seoul, Korea, Oct. 22-26, 2018. 
                     <a href="https://dl.acm.org/doi/abs/10.1145/3240508.3240557">[PDF]</a> 
                  </p>
                </li>

                <li>
                   <p align="justify">
                      <b>Xiangteng He</b>, Yuxin Peng, Junjie Zhao, 
                     "StackDRL: Stacked Deep Reinforcement Learning for Fine-grained Visual Categorization", 
                     International Joint Conference on Artificial Intelligence
                     (<b>IJCAI</b>), 
                     pp. 741-747, Stockholm, Sweden, Jul. 13-19, 2018.
                     <a href="https://ijcai.org/proceedings/2018/0103.pdf">[PDF]</a> 
                  </p>
                </li>

                <li>
                   <p align="justify">
                      Yuxin Peng, <b>Xiangteng He</b>, Junjie Zhao, 
                     "Object-Part Attention Model for Fine-grained Image Classification", 
                     IEEE Transactions on Image Processing
                     (<b>TIP</b>), 
                     Vol. 27, No. 3, pp. 1487-1500, Mar. 2018.
                     <a href="https://ieeexplore.ieee.org/abstract/document/8110709">[PDF]</a> 
                     <a href="https://github.com/PKU-ICST-MIPL/OPAM_TIP2018">[Code]</a>
                     <!--
                     <i style="color:red">
                      (This work was the highly cited paper by ESI, which received enough citations to place it in the top 1% of its academic field.)
                     </i>
                     -->
                  </p>
                </li>
              </ol>

            <h2>2017</h2>
              <ol>
                <li>
                   <p align="justify">
                      <b>Xiangteng He</b>, Yuxin Peng, Junjie Zhao, 
                     "Fine-grained Discriminative Localization via Saliency-guided Faster R-CNN", 
                     ACM Multimedia Conference
                     (<b>ACM MM</b>), 
                     pp. 627-635, Mountain View, CA, USA, Oct. 23-27, 2017.
                     <a href="https://dl.acm.org/doi/abs/10.1145/3123266.3123319">[PDF]</a> 
                     <a href="https://github.com/PKU-ICST-MIPL/Saliency-guided-Faster-R-CNN_ACMMM2017">[Code]</a> 
                  </p>
                </li>
                
                <li>
                   <p align="justify">
                      <b>Xiangteng He</b>, Yuxin Peng, 
                     "Fine-grained Image Classification via Combining Vision and Language", 
                     IEEE Conference on Computer Vision and Pattern Recognition
                     (<b>CVPR</b>), 
                     pp. 5994-6002, Honolulu, Hawaii, USA, Jul. 21-26, 2017.
                     <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/He_Fine-Grained_Image_Classification_CVPR_2017_paper.pdf">[PDF]</a> 
                  </p>
                </li>

                <li>
                   <p align="justify">
                      <b>Xiangteng He</b>, Yuxin Peng, 
                     "Weakly Supervised Learning of Part Selection Model with Spatial Constraints for Fine-grained Image Classification", 
                     AAAI Conference on Artificial Intelligence
                     (<b>AAAI</b>), 
                     pp. 4075-4081, San Francisco, California, USA, Feb. 4–9, 2017. 
                     <a href="https://ojs.aaai.org/index.php/AAAI/article/view/11223">[PDF]</a> 
                  </p>
                </li>
              </ol>

              
          <div class="col-xs-12">
    </div>
  </section>


  <section id="Honors" class="home-section">
    <div class="container">
      <h1>Honors</h1>
      <div class="col-xs-12">
        <li>Academic Innovation Award from Tencent WeChat (腾讯微信犀牛鸟专项研究计划学术创新奖), 2023</li>
        <li><a href="http://www.csig.org.cn/detail/3632">Young Elite Scientists Sponsorship Program by CAST, 2022</a>.</li>
        <li><a href="https://www.ccf.org.cn/Awards/Awards/2020-12-31/720619.shtml">CCF (China Computer Federation) Outstanding Doctoral Dissertation Award (10 in total), 2020</a>.</li>
        <li>Excellent Doctorate Dissertation Award of Peking University, 2020.</li>
        <li>Excellent Graduates of Beijing, 2020.</li>
        <li>Excellent Graduates of Peking University, 2020.</li>
        <li><a href="http://scholarship.baidu.com/">Baidu Scholarship</a> (6 in China, 10 in total of the world), 2018.</li>
        <li>National Scholarship (Top 1), 2018.</li>
        <li>Chancellor's Scholarship of Peking University, 2018.</li>
        <li>Top 10 Outstanding Academic Students, School of EECS, Peking University, 2018.</li>
      </div>
    </div>
  </section> 

  <!--
  <section id="Competitions" class="home-section">
    <div class="container">
      <h1>Competitions</h1>
      <div class="col-xs-12">
        <li>2014-11 As a member of ICST team to participate in TRECVID 2014, achieving first place in interactive instance search task.
        </li>

        <li>2015-11 As a member of ICST team to participate in TRECVID 2015, achieving first place in instance search task.
        </li>

        <li>2016-11 As a member of ICST team to participate in TRECVID 2016, achieving first place in instance search task.
        </li>
      </div>
    </div>
  </section> 
  -->
<!-- 
  <section id="Media Coverage" class="home-section">
    <div class="container">
      <h1>Media Coverage</h1>
      <div class="col-xs-12">
        <li>2020-12-31 CCF (China Computer Federation) Outstanding Doctoral Dissertation Award. 
          <a href="https://www.ccf.org.cn/Awards/Awards/2020-12-31/720619.shtml">[CCF News]</a> 
          <a href="https://mp.weixin.qq.com/s/hcCZX0fafsdnhCgshMEScA">[CCF CV News]</a>
          <a href="https://mp.weixin.qq.com/s/_MdFZsklAInzwtkm1teFpQ">[FITEE News]</a>
        </li>
        <li>2020-08-24 Excellent Doctorate Dissertation Award of Peking University. 
          <a href="https://www.wict.pku.edu.cn/xwgg/xwdt/2020/1329003.htm">[PKU WICT News]</a>
        </li>
        <li>2019-12-27 Science and Technology Youth.
          <a href="https://www.sohu.com/a/358891181_100118081">[Duxinshu News]</a>
        </li>
        <li>2019-10-16 PKU FG-XMedia: A New Benchmark for Fine-grained Cross-media Retrieval.
          <a href="https://www.jiqizhixin.com/articles/2019-10-15-14">[Synced News]</a>
        </li>
        <li>2018-12-28 Baidu Scholarship.
          <a href="https://mp.weixin.qq.com/s/q0c3pLu_-cVp921pJBlZGg">[Baidu News]</a>
          <a href="http://pku.cuepa.cn/show_more.php?tkey=&bkey=&doc_id=3060865">[PKU News]</a>
        </li>
      </div>
    </div>
  </section>  
-->

  





















  <div id="modal" class="modal fade" role="dialog">
    <div class="modal-dialog">
      <div class="modal-content">
        <div class="modal-header">
          <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
          <h4 class="modal-title">Cite</h4>
        </div>
        <div>
          <pre><code class="modal-body tex"></code></pre>
        </div>
        <div class="modal-footer">
          <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
            <i class="fa fa-copy"></i> Copy
          </a>
          <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
            <i class="fa fa-download"></i> Download
          </a>
          <div id="modal-error"></div>
        </div>
      </div>
    </div>
  </div>


    

    
    
    <script id="dsq-count-scr" src="//richardaecn.disqus.com/count.js" async></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    <script src="/js/hugo-academic.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script> 
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>
